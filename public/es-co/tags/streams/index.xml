<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>streams on Leandro Ordóñez Ante</title><link>https://leandro.ordonez.tech/es-co/tags/streams/</link><description>Recent content in streams on Leandro Ordóñez Ante</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 05 Apr 2020 14:10:33 +0100</lastBuildDate><atom:link href="https://leandro.ordonez.tech/es-co/tags/streams/index.xml" rel="self" type="application/rss+xml"/><item><title>Interactive querying for spatiotemporal data with Kafka Streams</title><link>https://leandro.ordonez.tech/es-co/ideas/interactive-querying-kafka-streams/</link><pubDate>Sun, 05 Apr 2020 14:10:33 +0100</pubDate><guid>https://leandro.ordonez.tech/es-co/ideas/interactive-querying-kafka-streams/</guid><description>&lt;figure class="center">&lt;img src="https://imgs.xkcd.com/comics/data_pipeline.png"
alt="Data Pipeline by xkcd.com"/>&lt;figcaption>
&lt;p>Data Pipeline by &lt;a href="https://xkcd.com/2054/">xkcd.com&lt;/a>&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>In a &lt;a href="https://leandro.ordonez.tech/ideas/continuous-aggregation-kafka-streams/">previous post&lt;/a> we explored a method for performing continuous aggregation on a stream of sensor readings using the Kafka Streams DSL. This time I want to share with you &lt;a href="https://github.com/LeandroOrdonez/kafka-streams-pipeline">&lt;code>kafka-streams-pipeline&lt;/code>&lt;/a>, a Kafka Streams application that leverages said continuous aggregation method and defines a complete stream processing pipeline, which enables querying the continuous data summaries stored into a materialized &lt;code>KTable&lt;/code>, by incorporating the spatial and temporal dimensions of the sensor data to the analysis. To understand how this application works, let’s first get back to the &lt;code>temperature-readings&lt;/code> topic from the previous post. In this topic the feed of temperature measurements made by multiple sensors is being posted. The following is an excerpt of the measurements registered in &lt;code>temperature-readings&lt;/code>:&lt;/p>
&lt;script type="application/javascript" src="https://gist.github.com/LeandroOrdonez/04eb02301d15f9af942cf76faa9e99c3.js?file=temperature_readings.json">&lt;/script>
&lt;p>Each JSON object from the list above represents a temperature reading taken by a given sensor. Notice the &lt;code>timestamp&lt;/code> and &lt;code>geohash&lt;/code> fields, which hold the information about the time and the location of each measurement. In case you are not familiar with the concept of geohashing, it is basically an encoding mechanism created by Gustavo Niemeyer in 2008, that allows reducing a two-dimensional longitude, latitude pair into a single alphanumeric string (in base32 format), where each subsequent character adds precision to the location. So, for instance, the geohash &lt;code>u155mz82dv33&lt;/code> corresponds to the longitude, latitude pair &lt;code>(4.47189873, 51.23760204)&lt;/code>. The idea is to enable interactive (low-latency) queries that support typical visual exploratory analysis tasks on top of the spatiotemporal data being posted to &lt;code>temperature-readings&lt;/code>, by leveraging some appealing features of geohashes such as its inherent hierarchical structure, which recursively subdivides geospatial areas into 32 cells or tiles at each level (character)&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, as you can see in the diagram below.&lt;/p>
&lt;figure class="img60">&lt;img src="https://leandro.ordonez.tech/images/ideas/geohash.jpg"
alt="Geohashes by Movable Type Scripts"/>&lt;figcaption>
&lt;p>Geohashes by &lt;a href="https://www.movable-type.co.uk/scripts/geohash.html">Movable Type Scripts&lt;/a>&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>This way, by truncating the &lt;code>geohash&lt;/code> of each of the incoming readings to a certain number characters, we can set up a grid for partitioning the temperature data in the spatial dimension, and perform continuous aggregation on each of the resulting partitions. The same principle can be applied in the temporal dimension by truncating the &lt;code>timestamp&lt;/code> of each reading to a certain unit of time, laying out a temporal partitioning schema that arranges data into time bins of a fixed width (for instance, &lt;em>minutely&lt;/em>, &lt;em>hourly&lt;/em>, or &lt;em>daily&lt;/em> bins).&lt;/p>
&lt;p>The application focuses on serving two types of queries: (&lt;em>1&lt;/em>) &lt;strong>Historical queries&lt;/strong>, namely those asking how the temperature has evolved along a given geospatial region, over a certain period of time; and (&lt;em>2&lt;/em>) &lt;strong>Snapshot queries&lt;/strong> which provide a time-slice view of the temperature for a specific moment in time.&lt;/p>
&lt;figure class="center">&lt;img src="https://leandro.ordonez.tech/images/ideas/historical-snapshot-queries.jpg"
alt="Historical and Snapshot queries"/>&lt;figcaption>
&lt;p>Historical and Snapshot queries&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>Below you can see a schematics showing the structure of the stream processing pipeline at the core of the application discussed in this post.&lt;/p>
&lt;figure class="img60">&lt;img src="https://leandro.ordonez.tech/images/ideas/architecture-kstreams-querying.jpg"
alt="Stream processing pipeline for interactive querying with Kafka Streams"/>&lt;figcaption>
&lt;p>Stream processing pipeline for interactive querying with Kafka Streams&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>As you can see in the diagram above, it is possible to deploy multiple stream processors, each one hosting an instance of the state store where one fraction of the global application state is persisted. In consequence, in this distributed setup queries are computed by combining the result sets obtained from each processor:&lt;/p>
&lt;figure class="center">&lt;img src="https://leandro.ordonez.tech/images/ideas/dist-query-processing.jpg"
alt="Distributed query resolution"/>&lt;figcaption>
&lt;p>Distributed query resolution&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>The source code of this application is available on &lt;a href="https://github.com/LeandroOrdonez/kafka-streams-pipeline">github&lt;/a>. You can deploy the entire pipeline (including an application for generating syntethic temperature values, discussed in a &lt;a href="https://leandro.ordonez.tech/ideas/mocking-sensor-data-generator/">recent post&lt;/a>) on your local machine using Docker. Just clone the repository and follow the instructions on the &lt;code>README.md&lt;/code> file. Hope you find it useful! :)&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>&lt;a href="https://www.movable-type.co.uk/scripts/geohash.html">Here&lt;/a> you can find a more detailed explanation on geohashes.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Mocking sensor data generator</title><link>https://leandro.ordonez.tech/es-co/ideas/mocking-sensor-data-generator/</link><pubDate>Wed, 01 Apr 2020 11:44:42 +0200</pubDate><guid>https://leandro.ordonez.tech/es-co/ideas/mocking-sensor-data-generator/</guid><description>&lt;figure>&lt;img src="https://leandro.ordonez.tech/images/ideas/map0-min.jpg"
alt="Photo by Thor Alvis on Unsplash"/>&lt;figcaption>
&lt;p>Photo by &lt;a href="https://unsplash.com/@terminath0r">Thor Alvis&lt;/a> on &lt;a href="https://unsplash.com/photos/dWwZBcjw3GE">Unsplash&lt;/a>&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>In case you are ever in need of some toy data to get you started with a data project, or just to test your ideas or implementations, I think you might find the Docker setup in &lt;a href="https://github.com/LeandroOrdonez/temp-readings-gen/">this repo&lt;/a> useful. It allows generating synthetic sensor readings of geolocated temperature data, and posting each record to a Kafka topic. The readings generated follow the format specified in the sample records below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>{&amp;#34;timestamp&amp;#34;: 1585054952527, &amp;#34;geohash&amp;#34;: &amp;#34;u155mz82dv33&amp;#34;, &amp;#34;sensorId&amp;#34;:&amp;#34;s000001&amp;#34;, &amp;#34;tempVal&amp;#34;: 20.3, &amp;#34;tempUnit&amp;#34;: &amp;#34;c&amp;#34;}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{&amp;#34;timestamp&amp;#34;: 1585080280353, &amp;#34;geohash&amp;#34;: &amp;#34;u155krxynu5s&amp;#34;, &amp;#34;sensorId&amp;#34;:&amp;#34;s000002&amp;#34;, &amp;#34;tempVal&amp;#34;: 19.7, &amp;#34;tempUnit&amp;#34;: &amp;#34;c&amp;#34;}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{&amp;#34;timestamp&amp;#34;: 1585080335267, &amp;#34;geohash&amp;#34;: &amp;#34;u155mz827m6q&amp;#34;, &amp;#34;sensorId&amp;#34;:&amp;#34;s000010&amp;#34;, &amp;#34;tempVal&amp;#34;: 24.6, &amp;#34;tempUnit&amp;#34;: &amp;#34;c&amp;#34;}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{&amp;#34;timestamp&amp;#34;: 1585080366564, &amp;#34;geohash&amp;#34;: &amp;#34;u155krxynuhu&amp;#34;, &amp;#34;sensorId&amp;#34;:&amp;#34;s000007&amp;#34;, &amp;#34;tempVal&amp;#34;: 17.2, &amp;#34;tempUnit&amp;#34;: &amp;#34;c&amp;#34;}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{&amp;#34;timestamp&amp;#34;: 1585080388268, &amp;#34;geohash&amp;#34;: &amp;#34;u155mz827v2n&amp;#34;, &amp;#34;sensorId&amp;#34;:&amp;#34;s000004&amp;#34;, &amp;#34;tempVal&amp;#34;: 22.2, &amp;#34;tempUnit&amp;#34;: &amp;#34;c&amp;#34;}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="usage">Usage&lt;/h2>
&lt;p>At least one running instance of Kafka and Zookeeper are required for the generator script to run. Optionally, we provide a &lt;code>docker-compose&lt;/code> file for deploying a testing Kafka environment in your local machine (based on the &lt;code>wurstmeister&lt;/code>&amp;rsquo;s &lt;a href="https://github.com/wurstmeister/kafka-docker/blob/master/docker-compose-single-broker.yml">kafka-docker&lt;/a> deployment). To use it, run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>docker-compose -f docker-compose-kafka.yml up -d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once this environment is up, you can spawn the Docker container where the generator script is going to run, and optionally specify some environment variables, such as the bounding box (&lt;code>BBOX&lt;/code>) within which the temperature readings are going to be located (as a comma-separated sequence of coordinates: &lt;code>&amp;lt;north&amp;gt;,&amp;lt;west&amp;gt;,&amp;lt;south&amp;gt;,&amp;lt;east&amp;gt;&lt;/code>), as well as a reference temperature value (&lt;code>AVG_TEMP&lt;/code>) the generated readings are going to oscillate around (in Celsius degrees):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>docker run -it \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --network=&amp;#34;host&amp;#34; \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -e KAFKA_BROKER=&amp;#34;localhost:9092&amp;#34; \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -e BBOX=&amp;#34;51.32288838086245,4.091720581054688,51.1509246836981,4.752960205078125&amp;#34; \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -e AVG_TEMP=19.0 \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -e KAFKA_TOPIC=&amp;#34;temperature-readings&amp;#34; \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -e RND_SEED=&amp;#34;0.0&amp;#34; \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> temp-readings-gen
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In case you already have a running Kafka setup, omit the &lt;code>--network=&amp;quot;host&amp;quot;&lt;/code> argument, and set the correspondent url (or urls) to the &lt;code>KAFKA_BROKER&lt;/code> environment variable.&lt;/p>
&lt;p>Now you should be able to consume the readings being posted to the &lt;code>temperature-readings&lt;/code> topic. You can test this by accessing the Kafka Docker container (or by &lt;code>ssh&lt;/code> into the broker of your existing Kafka setup) and running the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>kafka-console-consumer.sh \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --bootstrap-server localhost:9092 \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --topic temperature-readings \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --from-beginning \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --formatter kafka.tools.DefaultMessageFormatter \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --property print.value=true \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --property value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Below you can see the time-series chart of the typical values generated throughout one day:&lt;/p>
&lt;figure class="center">&lt;img src="https://leandro.ordonez.tech/images/ideas/daily-temp-fluctuation.png"
alt="Time-series chart of the generated temperature values"/>&lt;figcaption>
&lt;p>Time-series chart of the generated temperature values&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>And there you go, a running network of mobile sensors generating mock temperature data and pushing it into a Kafka topic. You can use this Docker setup to try, for instance, the Kafka streams application we discussed in &lt;a href="https://leandro.ordonez.tech/ideas/interactive-querying-kafka-streams/">this post&lt;/a>, allowing the continuous aggregation of a stream of sensor data.&lt;/p></description></item><item><title>Continuous aggregation in Kafka Streams</title><link>https://leandro.ordonez.tech/es-co/ideas/continuous-aggregation-kafka-streams/</link><pubDate>Thu, 17 Oct 2019 10:45:27 +0200</pubDate><guid>https://leandro.ordonez.tech/es-co/ideas/continuous-aggregation-kafka-streams/</guid><description>&lt;figure>&lt;img src="https://leandro.ordonez.tech/images/ideas/lago-bolson.jpg"
alt="Lago el Bolsón in Popayán, Colombia, my home town :)"/>&lt;figcaption>
&lt;p>Lago el Bolsón in Popayán, Colombia, my home town :)&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>In this post I show a method to address the computation of the arithmetic mean of an stream of values (say sensor readings) using the Kafka Streams DSL. The estimation of the average in a stream processing setting implies keeping track of other two measurements, namely the count of incoming records and the sum of their corresponding values. Let&amp;rsquo;s consider a Kafka streams application consuming messages from a topic to which the readings of multiple temperature sensors are being posted (&lt;code>temperature-readings&lt;/code>). The messages from said topic are keyed by the sensor ID, and we want to compute the rolling average of the temperature sensed by each device.&lt;/p>
&lt;p>Let&amp;rsquo;s first create a &lt;code>KGroupedStream&lt;/code> to group the sensor readings according to their corresponding sensor ID:&lt;/p>
&lt;script type="application/javascript" src="https://gist.github.com/LeandroOrdonez/aadca09b9aaa41a3b61c41796f3581d1.js?file=KafkaStreamsAggregator_1.java">&lt;/script>
&lt;p>Now we can use the &lt;code>KGroupedStream::aggregate&lt;/code> method to compute the rolling average on the &lt;code>perSensorStream&lt;/code> we got above. But before this, according to the Kafka &lt;a href="https://docs.confluent.io/current/streams/javadocs/org/apache/kafka/streams/kstream/KGroupedStream.html#aggregate-org.apache.kafka.streams.kstream.Initializer-org.apache.kafka.streams.kstream.Aggregator-">documentation&lt;/a> this method requires an &lt;code>Initializer&lt;/code> and an &lt;code>Aggregator&lt;/code> as arguments:&lt;/p>
&lt;blockquote>
&lt;p>The specified &lt;code>Initializer&lt;/code> is applied once directly before the first input record is processed to provide an initial intermediate aggregation result that is used to process the first record. The specified &lt;code>Aggregator&lt;/code> is applied for each input record and computes a new aggregate using the current aggregate (or for the very first record using the intermediate aggregation result provided via the Initializer) and the record&amp;rsquo;s value. Thus, aggregate(Initializer, Aggregator) can be used to compute aggregate functions like count (c.f. count()).&lt;/p>
&lt;/blockquote>
&lt;p>Let&amp;rsquo;s create a POJO to provide such initial aggregation and to hold the intermediate aggregation values:&lt;/p>
&lt;script type="application/javascript" src="https://gist.github.com/LeandroOrdonez/aadca09b9aaa41a3b61c41796f3581d1.js?file=AggregateTuple.java">&lt;/script>
&lt;p>Now we need to provide an implementation of the &lt;code>apply&lt;/code> method for the &lt;code>Aggregator&lt;/code> argument, which would be in charge of computing a new aggregate from the &lt;code>key&lt;/code> and &lt;code>value&lt;/code> of an incoming record and the current &lt;code>aggregate&lt;/code> of the same key:&lt;/p>
&lt;script type="application/javascript" src="https://gist.github.com/LeandroOrdonez/aadca09b9aaa41a3b61c41796f3581d1.js?file=KafkaStreamsAggregator_2.java">&lt;/script>
&lt;p>We can finally call the &lt;code>aggregate&lt;/code> method on the &lt;code>perSensorStream&lt;/code>:&lt;/p>
&lt;script type="application/javascript" src="https://gist.github.com/LeandroOrdonez/aadca09b9aaa41a3b61c41796f3581d1.js?file=KafkaStreamsAggregator_3.java">&lt;/script>
&lt;p>Below you can find the relevant code for the aggregation method outlined in this post.&lt;/p>
&lt;script type="application/javascript" src="https://gist.github.com/LeandroOrdonez/aadca09b9aaa41a3b61c41796f3581d1.js?file=KafkaStreamsAggregator.java">&lt;/script>
&lt;p>I didn&amp;rsquo;t include the code for handling serialization in this post, but you can find it here: &lt;a href="https://gist.github.com/LeandroOrdonez/aadca09b9aaa41a3b61c41796f3581d1#file-jsonpojoserializer-java">JsonPOJOSerializer&lt;/a> and &lt;a href="https://gist.github.com/LeandroOrdonez/aadca09b9aaa41a3b61c41796f3581d1#file-jsonpojodeserializer-java">JsonPOJODeserializer&lt;/a>.&lt;/p></description></item></channel></rss>